<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fang Zhao</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:64%;vertical-align:middle">
              <p style="text-align:left">
                <name>Fang Zhao</name>
              </p>
              <p> I am an Associate Professor at the School of Intelligence Science and Technology, Nanjing University. My research is currently on human-centric visual analysis and generationï¼Œ3D digital human.
                Previously, I was a senior researcher in <a href="https://ai.tencent.com/ailab/en/index/">Tencent AI Lab</a> from 2021 to 2023, and a research scientist in Inception Institute of Artificial Intelligence (IIAI) from 2018 to 2021,
                where I worked with Prof. <a href="https://shengcailiao.github.io/">Shengcai Liao</a>.
                I was a research fellow in National University of Singapore (NUS) from 2015 to 2017, co-supervised by Prof. <a href="https://sites.google.com/site/jshfeng/">Jiashi Feng</a> and Prof. <a href="https://yanshuicheng.info/">Shuicheng Yan</a>.
                I received my Ph.D. degree from Institute of Automation, Chinese Academy of Sciences (CASIA)</a>
                under the supervision of Prof. <a href="https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=en">Liang Wang</a>.
              </p>
              <!--<font color="red"><p><strong>Intern Positions at Tencent AI Lab:</strong> we are hiring self-motivated students as research interns,
                please feel free to drop me an email if you are interested.</p></font>-->
              <p style="text-align:left">
                <a href="mailto:zhaofang0627@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4C7mvOwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/zhaofang0627/">Github</a>
                
              </p>
            </td>
            <td style="padding:2.5%;width:44%;max-width:44%;vertical-align:middle">
              <img style="width:78%;max-width:78%" alt="profile photo" src="pics/photo.png">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/anchordef.gif' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Anchor_Transformations_for_3D_Garment_Animation_CVPR_2023_paper.pdf">
                <font color=#1772d0>  <papertitle>Learning Anchor Transformations for 3D Garment Animation</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Zekun Li,
              Shaoli Huang,
              Junwu Weng,
              Tianfei Zhou,
              Guo-Sen Xie,
              Jue Wang,
              Ying Shan
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2304.00761">arXiv</a> /
              <a href="https://semanticdh.github.io/AnchorDEF">Project</a> /
              <a href="bibs/anchor_def.bib">bibtex</a>
              <p></p>
              <p>This paper proposes an anchor-based deformation model, namely AnchorDEF, to predict 3D garment animation from a body motion sequence.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/R2ET.gif' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Skinned_Motion_Retargeting_With_Residual_Perception_of_Motion_Semantics__CVPR_2023_paper.pdf">
                <font color=#1772d0>  <papertitle>Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry</papertitle></font>
              </a>
              <br>
              Jiaxu Zhang,
              Junwu Weng,
              Di Kang,
              <strong>Fang Zhao</strong>,
              Shaoli Huang,
              Xuefei Zhe,
              Linchao Bao,
              Ying Shan,
              Jue Wang,
              Zhigang Tu
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.08658">arXiv</a> /
              <a href="https://semanticdh.github.io/R2ET">Project</a> /
              <a href="bibs/R2ET.bib">bibtex</a>
              <p></p>
              <p>R2ET is a neural motion retargeting model that can preserve source motion semantics and avoid interpenetration in target motion.</p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/HMC.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.10941.pdf">
                <font color=#1772d0>  <papertitle>HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting</papertitle></font>
              </a>
              <br>
              Haoyu Wang,
              Shaoli Huang,
              <strong>Fang Zhao</strong>,
              Chun Yuan,
              Ying Shan
              <br>
              <em>arXiv preprint.</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.10941">arXiv</a> /
              <a href="https://semanticdh.github.io/HMC">Project</a> /
              <a href="bibs/HMC.bib">bibtex</a>
              <p></p>
              <p>HMC shows a simple yet effective way of better handling local-part motions in the skeleton-free motion retaregting task.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/anchor_udf.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Learning_Anchored_Unsigned_Distance_Functions_With_Gradient_Direction_Alignment_for_ICCV_2021_paper.pdf">
                <font color=#1772d0>  <papertitle>Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for
                  Single-view Garment Reconstruction</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Wenhao Wang,
              Shengcai Liao,
              Ling Shao
              <br>
              <em>ICCV</em>, 2021 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2108.08478">arXiv</a> /
              <a href="https://github.com/zhaofang0627/AnchorUDF">Code</a> /
              <a href="bibs/anchor_udf.bib">bibtex</a>
              <p></p>
              <p>We propose a novel learnable Anchored Unsigned Distance Function (AnchorUDF)
                representation for 3D garment reconstruction from a single image.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/awb.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.06525.pdf">
                <font color=#1772d0>  <papertitle>Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain
                  Adaptation in Person Re-identification and Beyond</papertitle></font>
              </a>
              <br>
              Wenhao Wang,
              <strong>Fang Zhao</strong>,
              Shengcai Liao,
              Ling Shao
              <br>
              <em>TIP</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2006.06525">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/Attentive-WaveBlock">Code</a> /
              <a href="bibs/awb.bib">bibtex</a>
              <p></p>
              <p>This paper proposes a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated
                into the dual networks of mutual learning to enhance the complementarity.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/hpbtt.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://papers.nips.cc/paper/2020/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf">
                <font color=#1772d0>  <papertitle>Human Parsing Based Texture Transfer from Single Image to 3D Human via
                  Cross-View Consistency</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Shengcai Liao,
              Kaihao Zhang,
              Ling Shao
              <br>
              <em>NeurIPS</em>, 2020
              <br>
              <a href="https://github.com/zhaofang0627/HPBTT">Code</a> /
              <a href="bibs/hpbtt.bib">bibtex</a>
              <p></p>
              <p>A human parsing based texture transfer model via cross-view consistency learning which generates
                the texture of 3D human body from a single image.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/nrmt.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560511.pdf">
                <font color=#1772d0>  <papertitle>Unsupervised Domain Adaptation with Noise Resistible Mutual-Training
                  for Person Re-identification</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Shengcai Liao,
              Guo-Sen Xie,
              Jian Zhao,
              Kaihao Zhang,
              Ling Shao
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="bibs/nrmt.bib">bibtex</a>
              <p></p>
              <p>A Noise Resistible Mutual-Training (NRMT) method, which maintains two networks during training to perform
                collaborative clustering and mutual instance selection.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/dcn.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper.pdf">
                <font color=#1772d0>  <papertitle>Dynamic Conditional Networks for Few-Shot Learning</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao*</strong>,
              Jian Zhao*,
              Shuicheng Yan,
              Jiashi Feng (* - equal contribution)
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="https://github.com/ZhaoJ9014/Dynamic-Conditional-Networks.PyTorch">Code</a> /
              <a href="bibs/dcn.bib">bibtex</a>
              <p></p>
              <p>A novel Dynamic Conditional Convolutional Network (DCCN) is proposed to handle conditional few-shot learning,
                i.e, only a few training samples are available for each condition.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/deocc_lstm.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8101544">
                <font color=#1772d0>  <papertitle>Robust LSTM-Autoencoders for Face De-Occlusion in the Wild</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Jiashi Feng,
              Jian Zhao,
              Wenhan Yang,
              Shuicheng Yan
              <br>
              <em>TIP</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1612.08534">arXiv</a> /
              <a href="https://github.com/zhaofang0627/face-deocc-lstm">Code</a> /
              <a href="bibs/deocc_lstm.bib">bibtex</a>
              <p></p>
              <p>We propose a robust LSTM-Autoencoders (RLA) model consisting of two LSTM components, which aims at
                occlusion-robust face encoding and recurrent occlusion removal respectively. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/dsrh.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Deep_Semantic_Ranking_2015_CVPR_paper.pdf">
                <font color=#1772d0>  <papertitle>Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Yongzhen Huang,
              Liang Wang,
              Tieniu Tan
              <br>
              <em>CVPR</em>, 2015
              <br>
              <a href="https://arxiv.org/abs/1501.06272">arXiv</a> /
              <a href="https://github.com/zhaofang0627/cuda-convnet-for-hashing">Code</a> /
              <a href="bibs/dsrh.bib">bibtex</a>
              <p></p>
              <p>We propose a deep semantic ranking based method for learning hash functions that preserve
                multilevel semantic similarity between multi-label images. </p>
            </td>
          </tr>

        </tbody></table>
      </td>
    </tr>

  </table>


</body>

</html>
