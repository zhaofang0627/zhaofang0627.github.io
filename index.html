<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fang Zhao</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:64%;vertical-align:middle">
              <p style="text-align:left">
                <name>Fang Zhao</name>
              </p>
              <p>I am a Senior Researcher in <a href="https://ai.tencent.com/ailab/en/index/">Tencent AI Lab</a>. My research is currently on 3D vision and deep learning.
                Prior to Tencent, I was a research scientist in Inception Institute of Artificial Intelligence (IIAI)</a> from 2018 to 2021,
                where I worked with <a href="https://liaosc.wordpress.com/">Shengcai Liao</a> and
                <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">Ling Shao</a>.
                I was a research fellow in National University of Singapore (NUS) from 2015 to 2017, co-supervised by
                <a href="https://sites.google.com/site/jshfeng/">Jiashi Feng</a> and <a href="https://yanshuicheng.ai/">Shuicheng Yan</a>.
                I received my Ph.D. degree from Institute of Automation, Chinese Academy of Sciences (CASIA)</a>
                under the supervision of <a href="https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=en">Liang Wang</a>.
              </p>
              <p style="text-align:left">
                <a href="mailto:zhaofang0627@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4C7mvOwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/zhaofang0627/">Github</a>
                
              </p>
            </td>
            <td style="padding:2.5%;width:44%;max-width:44%;vertical-align:middle">
              <img style="width:78%;max-width:78%" alt="profile photo" src="pics/photo.png">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/anchor_udf.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Learning_Anchored_Unsigned_Distance_Functions_With_Gradient_Direction_Alignment_for_ICCV_2021_paper.pdf">
                <font color=#1772d0>  <papertitle>Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for
                  Single-view Garment Reconstruction</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Wenhao Wang,
              Shengcai Liao,
              Ling Shao
              <br>
              <em>ICCV</em>, 2021 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2108.08478">arXiv</a> /
              <a href="https://github.com/zhaofang0627/AnchorUDF">Code</a> /
              <a href="bibs/anchor_udf.bib">bibtex</a>
              <p></p>
              <p>We propose a novel learnable Anchored Unsigned Distance Function (AnchorUDF)
                representation for 3D garment reconstruction from a single image.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/hpbtt.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://papers.nips.cc/paper/2020/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf">
                <font color=#1772d0>  <papertitle>Human Parsing Based Texture Transfer from Single Image to 3D Human via
                  Cross-View Consistency</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Shengcai Liao,
              Kaihao Zhang,
              Ling Shao
              <br>
              <em>NeurIPS</em>, 2020
              <br>
              <a href="https://github.com/zhaofang0627/HPBTT">Code</a> /
              <a href="bibs/hpbtt.bib">bibtex</a>
              <p></p>
              <p>A human parsing based texture transfer model via cross-view consistency learning which generates
                the texture of 3D human body from a single image.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/dcn.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper.pdf">
                <font color=#1772d0>  <papertitle>Dynamic Conditional Networks for Few-Shot Learning</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao*</strong>,
              Jian Zhao*,
              Shuicheng Yan,
              Jiashi Feng (* - equal contribution)
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="https://github.com/ZhaoJ9014/Dynamic-Conditional-Networks.PyTorch">Code</a> /
              <a href="bibs/dcn.bib">bibtex</a>
              <p></p>
              <p>A novel Dynamic Conditional Convolutional Network (DCCN) is proposed to handle conditional few-shot learning,
                i.e, only a few training samples are available for each condition.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/deocc_lstm.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8101544">
                <font color=#1772d0>  <papertitle>Robust LSTM-Autoencoders for Face De-Occlusion in the Wild</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Jiashi Feng,
              Jian Zhao,
              Wenhan Yang,
              Shuicheng Yan
              <br>
              <em>TIP</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1612.08534">arXiv</a> /
              <a href="https://github.com/zhaofang0627/face-deocc-lstm">Code</a> /
              <a href="bibs/deocc_lstm.bib">bibtex</a>
              <p></p>
              <p>We propose a robust LSTM-Autoencoders (RLA) model consisting of two LSTM components, which aims at
                occlusion-robust face encoding and recurrent occlusion removal respectively. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/dsrh.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Deep_Semantic_Ranking_2015_CVPR_paper.pdf">
                <font color=#1772d0>  <papertitle>Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval</papertitle></font>
              </a>
              <br>
              <strong>Fang Zhao</strong>,
              Yongzhen Huang,
              Liang Wang,
              Tieniu Tan
              <br>
              <em>CVPR</em>, 2015
              <br>
              <a href="https://arxiv.org/abs/1501.06272">arXiv</a> /
              <a href="https://github.com/zhaofang0627/cuda-convnet-for-hashing">Code</a> /
              <a href="bibs/dsrh.bib">bibtex</a>
              <p></p>
              <p>We propose a deep semantic ranking based method for learning hash functions that preserve
                multilevel semantic similarity between multi-label images. </p>
            </td>
          </tr>

        </tbody></table>
      </td>
    </tr>

  </table>


</body>

</html>
